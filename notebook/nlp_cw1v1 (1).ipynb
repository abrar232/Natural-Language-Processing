{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9VwTEEQK2Xx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "! pip install datasets\n",
        "%pip install transformers\n",
        "%pip install spacy\n",
        "%pip install torch\n",
        "%pip install spacy-transformers\n",
        "%pip install transformers[torch]\n",
        "%pip install seqeval\n",
        "from datasets import load_dataset, load_metric\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from scipy.sparse import csr_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)"
      ],
      "metadata": {
        "id": "Nww974C4EBsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"surrey-nlp/PLOD-CW\")"
      ],
      "metadata": {
        "id": "ijmEov86JHic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.dataio import load_plod_cw, label_names, export_parquet, export_conll\n",
        "\n",
        "ds = load_plod_cw()\n",
        "\n",
        "print(ds)\n",
        "print(label_names(ds))"
      ],
      "metadata": {
        "id": "9DkwRazCnpTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_dataset = dataset[\"train\"][:200]\n",
        "val_dataset = dataset[\"validation\"]\n",
        "test_dataset = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "olmGvTojEapd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "tokens = short_dataset[\"tokens\"]\n",
        "ner_tags = short_dataset[\"ner_tags\"]\n",
        "\n",
        "df = pd.DataFrame({\"tokens\": tokens, \"ner_tags\": ner_tags})\n",
        "\n",
        "ner_counter = Counter()\n",
        "for tags in ner_tags:\n",
        "  for tag in tags:\n",
        "    ner_counter[tag] += 1\n",
        "\n",
        "plt.bar(ner_counter.keys(), ner_counter.values())\n",
        "plt.xlabel(\"NER Type\")\n",
        "plt.ylabel(\"Occurances\")\n",
        "plt.title(\"NER Distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AIQ0oaA9sQB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_input = tokenizer(short_dataset[\"tokens\"], is_split_into_words=True)"
      ],
      "metadata": {
        "id": "JBSuCgv5EePo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoding = {\"B-O\": 0, \"B-AC\": 1, \"B-LF\": 2, \"I-LF\": 3}\n",
        "\n",
        "label_list = []\n",
        "for sample in short_dataset[\"ner_tags\"]:\n",
        "    label_list.append([label_encoding[tag] for tag in sample])\n",
        "\n",
        "val_label_list = []\n",
        "for sample in val_dataset[\"ner_tags\"]:\n",
        "    val_label_list.append([label_encoding[tag] for tag in sample])\n",
        "\n",
        "test_label_list = []\n",
        "for sample in test_dataset[\"ner_tags\"]:\n",
        "    test_label_list.append([label_encoding[tag] for tag in sample])"
      ],
      "metadata": {
        "id": "KVyyX7ViEiUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(short_dataset, list_name):\n",
        "    tokenized_inputs = tokenizer(short_dataset[\"tokens\"], truncation=True, is_split_into_words=True) ## For some models, you may need to set max_length to approximately 500.\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(list_name):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            else:\n",
        "                label_ids.append(label[word_idx])\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "LUlyoPNOEmNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = tokenize_and_align_labels(short_dataset, label_list)\n",
        "tokenized_val_datasets = tokenize_and_align_labels(val_dataset, val_label_list)\n",
        "tokenized_test_datasets = tokenize_and_align_labels(test_dataset, test_label_list)"
      ],
      "metadata": {
        "id": "hVsWL14REpS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def turn_dict_to_list_of_dict(d):\n",
        "    new_list = []\n",
        "\n",
        "    for labels, inputs in zip(d[\"labels\"], d[\"input_ids\"]):\n",
        "        entry = {\"input_ids\": inputs, \"labels\": labels}\n",
        "        new_list.append(entry)\n",
        "\n",
        "    return new_list"
      ],
      "metadata": {
        "id": "8HiTc4nEEsm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenised_train = turn_dict_to_list_of_dict(tokenized_datasets)\n",
        "tokenised_val = turn_dict_to_list_of_dict(tokenized_val_datasets)\n",
        "tokenised_test = turn_dict_to_list_of_dict(tokenized_test_datasets)"
      ],
      "metadata": {
        "id": "cPfc_6PwEvon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ],
      "metadata": {
        "id": "GNauUFXlEyal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "metric = load_metric(\"seqeval\")\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "qy_uUSflE6nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "\n",
        "# Training arguments (feel free to play arround with these values)\n",
        "model_name = \"bert-base-uncased\"\n",
        "epochs = 6\n",
        "batch_size = 4\n",
        "learning_rate = 2e-5\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"BERT-finetuned-NER\",\n",
        "    # evaluation_strategy = \"epoch\", ## Instead of focusing on loss and accuracy, we will focus on the F1 score\n",
        "    evaluation_strategy ='steps',\n",
        "    eval_steps = 7000,\n",
        "    save_total_limit = 3,\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epochs,\n",
        "    weight_decay=0.001,\n",
        "    save_steps=35000,\n",
        "    metric_for_best_model = 'f1',\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenised_train,\n",
        "    eval_dataset=tokenised_val,\n",
        "    data_collator = data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")"
      ],
      "metadata": {
        "id": "diwRrtbmHUgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "65sZUfPWHc-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the test data for evaluation in the same format as the training data\n",
        "\n",
        "predictions, labels, _ = trainer.predict(tokenised_test)\n",
        "predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "# Remove the predictions for the [CLS] and [SEP] tokens\n",
        "true_predictions = [\n",
        "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "\n",
        "# Compute multiple metrics on the test restuls\n",
        "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "results\n",
        "\n"
      ],
      "metadata": {
        "id": "uUgnsKnRMeNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sparse_metrics(true_predictions, true_labels):\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    precision = results[\"overall_precision\"]\n",
        "    recall = results[\"overall_recall\"]\n",
        "    f1 = results[\"overall_f1\"]\n",
        "    accuracy = results[\"overall_accuracy\"]\n",
        "\n",
        "    metrics = csr_matrix([[precision, recall, f1, accuracy]])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "test_results = sparse_metrics(true_predictions, true_labels)\n",
        "\n",
        "precision = test_results[0, 0]\n",
        "recall = test_results[0, 1]\n",
        "f1 = test_results[0, 2]\n",
        "accuracy = test_results[0, 3]\n",
        "\n",
        "metrics = [\"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"]\n",
        "\n",
        "dense_results = test_results.toarray()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(metrics, dense_results[0], color=['blue', 'green', 'orange', 'red'])\n",
        "plt.title('Performance Metrics')\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "metrics_df = pd.DataFrame(dense_results, columns=metrics)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(metrics_df, annot=True, cmap=\"Blues\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Performance Metrics\")\n",
        "plt.xlabel(\"Metrics\")\n",
        "plt.ylabel(\"Model\")\n",
        "plt.yticks([0], ['Model 1'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QXDBxB2XqBYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyper-Parameter optimization**"
      ],
      "metadata": {
        "id": "9_s3ggtRv5Fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lower learning rate value, lower Batch size, lower epochs"
      ],
      "metadata": {
        "id": "jw437DQOwAU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "\n",
        "# Training arguments (feel free to play arround with these values)\n",
        "model_name = \"bert-base-uncased\"\n",
        "epochs = 3\n",
        "batch_size = 2\n",
        "learning_rate = 1e-5\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"BERT-finetuned-NER\",\n",
        "    # evaluation_strategy = \"epoch\", ## Instead of focusing on loss and accuracy, we will focus on the F1 score\n",
        "    evaluation_strategy ='steps',\n",
        "    eval_steps = 7000,\n",
        "    save_total_limit = 3,\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epochs,\n",
        "    weight_decay=0.001,\n",
        "    save_steps=35000,\n",
        "    metric_for_best_model = 'f1',\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenised_train,\n",
        "    eval_dataset=tokenised_val,\n",
        "    data_collator = data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")"
      ],
      "metadata": {
        "id": "i3BUkgFVv3yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "CW2zT2nexFFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the test data for evaluation in the same format as the training data\n",
        "\n",
        "predictions, labels, _ = trainer.predict(tokenised_test)\n",
        "predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "# Remove the predictions for the [CLS] and [SEP] tokens\n",
        "true_predictions = [\n",
        "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "\n",
        "# Compute multiple metrics on the test restuls\n",
        "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "results\n"
      ],
      "metadata": {
        "id": "RKBAPVaAxJIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_metrics(true_predictions, true_labels):\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    precision = results[\"overall_precision\"]\n",
        "    recall = results[\"overall_recall\"]\n",
        "    f1 = results[\"overall_f1\"]\n",
        "    accuracy = results[\"overall_accuracy\"]\n",
        "\n",
        "    metrics = csr_matrix([[precision, recall, f1, accuracy]])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "test_results = sparse_metrics(true_predictions, true_labels)\n",
        "\n",
        "precision = test_results[0, 0]\n",
        "recall = test_results[0, 1]\n",
        "f1 = test_results[0, 2]\n",
        "accuracy = test_results[0, 3]\n",
        "\n",
        "metrics = [\"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"]\n",
        "\n",
        "dense_results = test_results.toarray()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(metrics, dense_results[0], color=['blue', 'green', 'orange', 'red'])\n",
        "plt.title('Performance Metrics')\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "metrics_df = pd.DataFrame(dense_results, columns=metrics)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(metrics_df, annot=True, cmap=\"Blues\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Performance Metrics\")\n",
        "plt.xlabel(\"Metrics\")\n",
        "plt.ylabel(\"Model\")\n",
        "plt.yticks([0], ['Model 1'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GIAwTocKxOw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Higher learning rate value, lower Batch size, lower epochs"
      ],
      "metadata": {
        "id": "mShugDWq1FX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments (feel free to play arround with these values)\n",
        "model_name = \"bert-base-uncased\"\n",
        "epochs = 3\n",
        "batch_size = 2\n",
        "learning_rate = 4e-5\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"BERT-finetuned-NER\",\n",
        "    # evaluation_strategy = \"epoch\", ## Instead of focusing on loss and accuracy, we will focus on the F1 score\n",
        "    evaluation_strategy ='steps',\n",
        "    eval_steps = 7000,\n",
        "    save_total_limit = 3,\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epochs,\n",
        "    weight_decay=0.001,\n",
        "    save_steps=35000,\n",
        "    metric_for_best_model = 'f1',\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenised_train,\n",
        "    eval_dataset=tokenised_val,\n",
        "    data_collator = data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")"
      ],
      "metadata": {
        "id": "8-rutzFs1OeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "1fBdorr-1XPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the test data for evaluation in the same format as the training data\n",
        "\n",
        "predictions, labels, _ = trainer.predict(tokenised_test)\n",
        "predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "# Remove the predictions for the [CLS] and [SEP] tokens\n",
        "true_predictions = [\n",
        "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "\n",
        "# Compute multiple metrics on the test restuls\n",
        "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "results"
      ],
      "metadata": {
        "id": "VKPQCNFt1X0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_metrics(true_predictions, true_labels):\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    precision = results[\"overall_precision\"]\n",
        "    recall = results[\"overall_recall\"]\n",
        "    f1 = results[\"overall_f1\"]\n",
        "    accuracy = results[\"overall_accuracy\"]\n",
        "\n",
        "    metrics = csr_matrix([[precision, recall, f1, accuracy]])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "test_results = sparse_metrics(true_predictions, true_labels)\n",
        "\n",
        "precision = test_results[0, 0]\n",
        "recall = test_results[0, 1]\n",
        "f1 = test_results[0, 2]\n",
        "accuracy = test_results[0, 3]\n",
        "\n",
        "metrics = [\"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"]\n",
        "\n",
        "dense_results = test_results.toarray()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(metrics, dense_results[0], color=['blue', 'green', 'orange', 'red'])\n",
        "plt.title('Performance Metrics')\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "metrics_df = pd.DataFrame(dense_results, columns=metrics)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(metrics_df, annot=True, cmap=\"Blues\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Performance Metrics\")\n",
        "plt.xlabel(\"Metrics\")\n",
        "plt.ylabel(\"Model\")\n",
        "plt.yticks([0], ['Model 1'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iFdSLgRu1bNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Higher learning rate value, lower Batch size, higher epochs"
      ],
      "metadata": {
        "id": "eqxjCHx04yh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments (feel free to play arround with these values)\n",
        "model_name = \"bert-base-uncased\"\n",
        "epochs = 12\n",
        "batch_size = 2\n",
        "learning_rate = 4e-5\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"BERT-finetuned-NER\",\n",
        "    # evaluation_strategy = \"epoch\", ## Instead of focusing on loss and accuracy, we will focus on the F1 score\n",
        "    evaluation_strategy ='steps',\n",
        "    eval_steps = 7000,\n",
        "    save_total_limit = 3,\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epochs,\n",
        "    weight_decay=0.001,\n",
        "    save_steps=35000,\n",
        "    metric_for_best_model = 'f1',\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenised_train,\n",
        "    eval_dataset=tokenised_val,\n",
        "    data_collator = data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")"
      ],
      "metadata": {
        "id": "5X6yZVja47vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "i7P4QA0O5M2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the test data for evaluation in the same format as the training data\n",
        "\n",
        "predictions, labels, _ = trainer.predict(tokenised_test)\n",
        "predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "# Remove the predictions for the [CLS] and [SEP] tokens\n",
        "true_predictions = [\n",
        "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "\n",
        "# Compute multiple metrics on the test restuls\n",
        "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "results"
      ],
      "metadata": {
        "id": "4YwNmqoC5DTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_metrics(true_predictions, true_labels):\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    precision = results[\"overall_precision\"]\n",
        "    recall = results[\"overall_recall\"]\n",
        "    f1 = results[\"overall_f1\"]\n",
        "    accuracy = results[\"overall_accuracy\"]\n",
        "\n",
        "    metrics = csr_matrix([[precision, recall, f1, accuracy]])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "test_results = sparse_metrics(true_predictions, true_labels)\n",
        "\n",
        "precision = test_results[0, 0]\n",
        "recall = test_results[0, 1]\n",
        "f1 = test_results[0, 2]\n",
        "accuracy = test_results[0, 3]\n",
        "\n",
        "metrics = [\"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"]\n",
        "\n",
        "dense_results = test_results.toarray()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(metrics, dense_results[0], color=['blue', 'green', 'orange', 'red'])\n",
        "plt.title('Performance Metrics')\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "metrics_df = pd.DataFrame(dense_results, columns=metrics)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(metrics_df, annot=True, cmap=\"Blues\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Performance Metrics\")\n",
        "plt.xlabel(\"Metrics\")\n",
        "plt.ylabel(\"Model\")\n",
        "plt.yticks([0], ['Model 1'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h8CYr-MP5G29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Higher learning rate value, higher Batch size, higher epochs"
      ],
      "metadata": {
        "id": "8Iprx91qEpui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments (feel free to play arround with these values)\n",
        "model_name = \"bert-base-uncased\"\n",
        "epochs = 12\n",
        "batch_size = 8\n",
        "learning_rate = 4e-5\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"BERT-finetuned-NER\",\n",
        "    # evaluation_strategy = \"epoch\", ## Instead of focusing on loss and accuracy, we will focus on the F1 score\n",
        "    evaluation_strategy ='steps',\n",
        "    eval_steps = 7000,\n",
        "    save_total_limit = 3,\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epochs,\n",
        "    weight_decay=0.001,\n",
        "    save_steps=35000,\n",
        "    metric_for_best_model = 'f1',\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenised_train,\n",
        "    eval_dataset=tokenised_val,\n",
        "    data_collator = data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")"
      ],
      "metadata": {
        "id": "SSl8H0KVEvDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "bEgwhso1E0UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the test data for evaluation in the same format as the training data\n",
        "\n",
        "predictions, labels, _ = trainer.predict(tokenised_test)\n",
        "predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "# Remove the predictions for the [CLS] and [SEP] tokens\n",
        "true_predictions = [\n",
        "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "\n",
        "# Compute multiple metrics on the test restuls\n",
        "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "results"
      ],
      "metadata": {
        "id": "Xbz4QzlME3vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_metrics(true_predictions, true_labels):\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    precision = results[\"overall_precision\"]\n",
        "    recall = results[\"overall_recall\"]\n",
        "    f1 = results[\"overall_f1\"]\n",
        "    accuracy = results[\"overall_accuracy\"]\n",
        "\n",
        "    metrics = csr_matrix([[precision, recall, f1, accuracy]])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "test_results = sparse_metrics(true_predictions, true_labels)\n",
        "\n",
        "precision = test_results[0, 0]\n",
        "recall = test_results[0, 1]\n",
        "f1 = test_results[0, 2]\n",
        "accuracy = test_results[0, 3]\n",
        "\n",
        "metrics = [\"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"]\n",
        "\n",
        "dense_results = test_results.toarray()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(metrics, dense_results[0], color=['blue', 'green', 'orange', 'red'])\n",
        "plt.title('Performance Metrics')\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "metrics_df = pd.DataFrame(dense_results, columns=metrics)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(metrics_df, annot=True, cmap=\"Blues\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Performance Metrics\")\n",
        "plt.xlabel(\"Metrics\")\n",
        "plt.ylabel(\"Model\")\n",
        "plt.yticks([0], ['Model 1'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6VOE-I_4E8bH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}